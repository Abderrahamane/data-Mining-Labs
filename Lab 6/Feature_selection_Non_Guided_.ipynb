{"cells":[{"cell_type":"markdown","metadata":{"id":"pMFQ8jWRYab5"},"source":["# LAB 6 : Feature selection methods\n","All the different approaches to feature selection can be grouped into mainly two families of methods. There are unsupervised and supervised methods. The latter can be further divided into the wrapper and filter mothods. Let’s discuss them one by one.\n","\n","In this lab, your task is to carefully review all instructions and fill the empty code cells with the necessary code to ensure everything functions correctly.\n"]},{"cell_type":"markdown","metadata":{"id":"SZeKU-tTYab_"},"source":["## Import necessary libraries"]},{"cell_type":"markdown","metadata":{"id":"S4wp4qNHYacB"},"source":["Requirement:\n","- pip install mlxtend\n","- pip install sklearn-genetic-opt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27241,"status":"ok","timestamp":1730563143185,"user":{"displayName":"Sami Belkacem","userId":"10047842610731585210"},"user_tz":-60},"id":"SrUnp-q9Qz9z"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n","Requirement already satisfied: scipy\u003e=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.3)\n","Requirement already satisfied: numpy\u003e=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n","Requirement already satisfied: pandas\u003e=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n","Requirement already satisfied: scikit-learn\u003e=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n","Requirement already satisfied: matplotlib\u003e=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n","Requirement already satisfied: joblib\u003e=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.2)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.0.0-\u003emlxtend) (1.3.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.0.0-\u003emlxtend) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.0.0-\u003emlxtend) (4.60.1)\n","Requirement already satisfied: kiwisolver\u003e=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.0.0-\u003emlxtend) (1.4.9)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.0.0-\u003emlxtend) (25.0)\n","Requirement already satisfied: pillow\u003e=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.0.0-\u003emlxtend) (11.3.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.0.0-\u003emlxtend) (3.2.5)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.0.0-\u003emlxtend) (2.9.0.post0)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=0.24.2-\u003emlxtend) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas\u003e=0.24.2-\u003emlxtend) (2025.2)\n","Requirement already satisfied: threadpoolctl\u003e=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn\u003e=1.3.1-\u003emlxtend) (3.6.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib\u003e=3.0.0-\u003emlxtend) (1.17.0)\n","Collecting sklearn-genetic-opt\n","  Downloading sklearn_genetic_opt-0.12.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: scikit-learn\u003e=1.5.0 in /usr/local/lib/python3.12/dist-packages (from sklearn-genetic-opt) (1.6.1)\n","Requirement already satisfied: numpy\u003e=1.26.1 in /usr/local/lib/python3.12/dist-packages (from sklearn-genetic-opt) (2.0.2)\n","Collecting deap\u003e=1.3.3 (from sklearn-genetic-opt)\n","  Downloading deap-1.4.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: tqdm\u003e=4.61.1 in /usr/local/lib/python3.12/dist-packages (from sklearn-genetic-opt) (4.67.1)\n","Requirement already satisfied: scipy\u003e=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn\u003e=1.5.0-\u003esklearn-genetic-opt) (1.16.3)\n","Requirement already satisfied: joblib\u003e=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn\u003e=1.5.0-\u003esklearn-genetic-opt) (1.5.2)\n","Requirement already satisfied: threadpoolctl\u003e=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn\u003e=1.5.0-\u003esklearn-genetic-opt) (3.6.0)\n","Downloading sklearn_genetic_opt-0.12.0-py3-none-any.whl (37 kB)\n","Downloading deap-1.4.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: deap, sklearn-genetic-opt\n","Successfully installed deap-1.4.3 sklearn-genetic-opt-0.12.0\n"]}],"source":["!pip install mlxtend\n","!pip install sklearn-genetic-opt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TkxOJtU0zt02","scrolled":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import time\n","import warnings\n","\n","from sklearn.datasets import load_iris, load_breast_cancer, fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, mutual_info_classif, f_classif\n","from sklearn.feature_selection import RFE\n","from sklearn.metrics import accuracy_score\n","from mlxtend.feature_selection import SequentialFeatureSelector\n","from sklearn_genetic import GAFeatureSelectionCV\n","warnings.filterwarnings(\"ignore\")\n"]},{"cell_type":"markdown","metadata":{"id":"qCeFCLLOjWyo"},"source":["# About the dataset\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x1z5QCKAmK1g"},"source":["**The Forest Cover Type Dataset (Covertype)** contains data on forest cover types based on cartographic variables.\n","\n","It is composed of 110,393 instances and 54 features (including continuous, categorical, and binary features) that represent various environmental factors, making it ideal for exploring feature selection.\n","\n","The classification task is to predict the type of forest cover (seven possible types).\n","\n","Here is a description of each forest cover class in the used dataset:\n","\n","**Spruce/Fir (Class 1):** A forest type primarily composed of spruce and fir trees, which are common in colder climates and higher elevations. These trees are known for their needle-like leaves and are typically evergreen.\n","\n","**Lodgepole Pine (Class 2):** Dominated by lodgepole pine trees, which grow well in diverse soil types and are highly adaptable. Lodgepole pines are resilient, often thriving in areas prone to wildfires as their cones require heat to open and release seeds.\n","\n","**Ponderosa Pine (Class 3):** Ponderosa pine forests are found at lower elevations and warmer climates. These trees are tall with thick bark and long needles, often providing valuable habitats and resources for wildlife.\n","\n","**Cottonwood/Willow (Class 4):** A mix of cottonwood and willow trees, often found near water sources like rivers or lakes due to their preference for moist soils. These trees are deciduous, shedding leaves seasonally, and play an essential role in riparian ecosystems.\n","\n","**Aspen (Class 5):** Aspen forests consist mainly of aspen trees, known for their smooth, white bark and leaves that tremble in the wind. Aspens grow in clonal colonies and are usually found in cooler, moist environments.\n","\n","**Douglas-fir (Class 6):** A forest type dominated by Douglas-fir trees, which are common in the western U.S. These tall trees with conical shapes are important for the timber industry and thrive in both dry and wet climates.\n","\n","**Krummholz (Class 7):** Known as \"twisted wood,\" Krummholz forests are made up of trees that are stunted and gnarled due to harsh environmental conditions, such as strong winds and cold temperatures. They are commonly found at the treeline in mountainous areas."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":4759,"status":"ok","timestamp":1730563179681,"user":{"displayName":"Sami Belkacem","userId":"10047842610731585210"},"user_tz":-60},"id":"Ys4eV2Dr0QOE"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (ipython-input-2789073567.py, line 10)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2789073567.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    continuous_features = initial_data.#FIX ME#\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["# Load the dataset\n","data = fetch_openml(\"Covertype\", version=1)\n","\n","initial_data = pd.DataFrame(data.data)\n","initial_data['class'] = data.target\n","\n","initial_data.dtypes\n","\n","#select Continuous and Categorical features\n","continuous_features = initial_data.#FIX ME#\n","categorical_features = initial_data.#FIX ME# Careful not to include target feature\n","\n","#Print their shape\n","#Fix ME#"]},{"cell_type":"markdown","metadata":{"id":"i5XKg6duqdr0"},"source":["# Data sampling\n","\n","For learning purposes, and due to the large size of the Covertype dataset, using the entire dataset during the lab session would be impractical. Instead, we will work with a sample to make our analysis and modeling more manageable. To ensure that our sample reflects the original distribution of the target variable, we will perform **stratified sampling**. This approach will allow us to preserve the relative proportions of each forest cover type within our sample, ensuring that our insights and model results remain representative of the full dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5,"status":"ok","timestamp":1730563179681,"user":{"displayName":"Sami Belkacem","userId":"10047842610731585210"},"user_tz":-60},"id":"WiYHDEMkrasA"},"outputs":[],"source":["# Create a 1% stratified sample from the original dataset ensuring reproducibility\n","X_sampled, _ = train_test_split(initial_data, #FIX ME#, #FIX ME#, #FIX ME#)\n","print(X_sampled.shape)"]},{"cell_type":"markdown","metadata":{"id":"Hhx_GNpsTwlS"},"source":["Display a Bar chart to visualize the class distribution in the dataset before and after performing the stratified sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":893,"status":"ok","timestamp":1730563180571,"user":{"displayName":"Sami Belkacem","userId":"10047842610731585210"},"user_tz":-60},"id":"I9M6M1uzTnr3"},"outputs":[],"source":["plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","initial_data['class'].value_counts().plot(kind='bar', color='#4287f5')\n","plt.title('Original Class Distribution')\n","plt.xlabel('Class')\n","plt.ylabel('Frequency')\n","\n","# Sampled class distribution\n","plt.subplot(1, 2, 2)\n","X_sampled['class'].value_counts().plot(kind='bar', color='#FF5733')\n","plt.title('Sampled Class Distribution')\n","plt.xlabel('Class')\n","plt.ylabel('Frequency')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":10,"status":"ok","timestamp":1730563180572,"user":{"displayName":"Sami Belkacem","userId":"10047842610731585210"},"user_tz":-60},"id":"Aq6UNau9rNcO"},"outputs":[],"source":["#Splitting the sampled dataset into features and target\n","X = X_sampled.iloc[:,:-1]\n","y = pd.Series(X_sampled['class'], name='target')\n","print(f\"Number of Features : {X.shape[1]}\")\n","print(f\"Sample of Featues values : \\n {X.head}\")\n","print(\"#############################\")\n","print(f\"Sample of Target values : \\n {y.head}\")"]},{"cell_type":"markdown","metadata":{"id":"tyWhUKeVYacF"},"source":["## Split the dataset into training and testing sets"]},{"cell_type":"markdown","metadata":{"id":"qkKXgtWihFkV"},"source":["**We need to explain what is spliting and why do we need it (model evaluation):**\n","\n","In this classification problem, our goal is to develop a model that can accurately classify flowers based on given features. To achieve this, we need to follow a few critical steps:\n","\n","(1) *Data Preprocessing* (already done).\n","\n","(2) **Splitting the Dataset:** We divide the dataset into two subsets—the Training Set and the Testing Set.\n","\n","(3) **Model Training:** Using the training set, we provide the model with pairs of features and their respective class labels, enabling it to learn patterns in the data and understand when to assign certain classes to specific examples (rows).\n","\n","(4) **Model Testing:** Once training is complete, we evaluate the model by testing it on the testing set. This assessment helps us determine how well the model generalizes to new, unseen data (i.e., can the model correctly classify flowers it hasn’t encountered during training?).\n","\n","### **Why Splitting Is Necessary**\n","\n","Splitting the data is crucial for reliable model evaluation. By keeping part of the data aside for testing, we can better assess the model's ability to generalize to new data, which reflects real-world scenarios. If we were to train and test on the same data, the model might perform well simply because it has **\"memorized\"** the data, rather than learned **generalizable patterns**. This could lead to overfitting, where the model fails on unseen data.\n","\n","**Note: For this lab, we’ll use accuracy as the performance metric. However, there are several other metrics that can provide a more comprehensive evaluation of a model’s performance, especially in cases with imbalanced classes.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1730563180572,"user":{"displayName":"Sami Belkacem","userId":"10047842610731585210"},"user_tz":-60},"id":"aae0fp6r0XiB"},"outputs":[],"source":["#Split the data set into two subsets, 80% for training and 20% for testing.\n","X_train, X_test, y_train, y_test = train_test_split(\n","                                                    X,\n","                                                    y,\n","                                                    #FIX ME#,\n","                                                    #FIX ME#\n","                                                    )\n","\n","print(f\"Training set's shape : {X_train.shape}\")\n","print(f\"Targets training set's shape : {y_train.shape}\")\n","print(\"####################################################\")\n","print(f\"Testing set's shape : {X_test.shape}\")\n","print(f\"Targets testing set's shape : {y_test.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"zxbZOYv4vrmc"},"source":["Now, to the Feature selection methods, as mentioned before, two main categories of methods could be distinguished: Unsupervised and Supervised"]},{"cell_type":"markdown","metadata":{"id":"EJ-aeFjwYacI"},"source":["# Unsupervised feature selection methods\n"]},{"cell_type":"markdown","metadata":{"id":"xrxIG5_Iwh_5"},"source":["In this section we will see three different methods; **Variance threshold**, **Correlation** and **Mutual information**"]},{"cell_type":"markdown","metadata":{"id":"h6L6kwkMYacI"},"source":["## Variance Threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VnoLwXjU1uaI"},"outputs":[],"source":["def variance_threshold_selector(X, threshold=0.0):\n","    selector = VarianceThreshold(#FIX ME#)\n","    selector.fit(X)\n","    return X[X.columns[selector.get_support(indices=True)]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1730563185951,"user":{"displayName":"Sami Belkacem","userId":"10047842610731585210"},"user_tz":-60},"id":"eCcofAc84JxY"},"outputs":[],"source":["print(f\"Initial features: \\n{X.columns.shape}, {X.columns.values}\")\n","X_var = variance_threshold_selector(\n","    #FIX ME#,\n","    threshold=0.1\n","    )\n","print(f\"Selected features Shape and names: \\n{X_var.columns.shape}, {X_var.columns.values}\")"]},{"cell_type":"markdown","metadata":{"id":"Wo0TEvvlYacK"},"source":["## Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":887},"executionInfo":{"elapsed":1629,"status":"ok","timestamp":1730563196122,"user":{"displayName":"Sami Belkacem","userId":"10047842610731585210"},"user_tz":-60},"id":"htB5zU3uhFkW"},"outputs":[],"source":["cor = #FIX ME#.corr().abs()\n","plt.figure(figsize=(12, 8))\n","sns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HhZgJFdL7Vlr"},"outputs":[],"source":["def correlation_selector(X, threshold=0.8):\n","    corr_matrix = #FIX ME#.corr().abs()\n","    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","    to_drop = [column for column in upper.columns if any(upper[column] \u003e threshold)]\n","    return X.drop(to_drop, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"HiuVYLKDU94N"},"outputs":[],"source":["print(f\"Initial features: \\n{X.columns.values}\")\n","#Set timer start\n","start_time = #FIX ME#\n","X_corr = correlation_selector(X_train[continuous_features], threshold=0.7)\n","#Set timer end\n","end_time = #FIX ME#\n","print(f\"Selected features: \\n{X_corr.columns.shape}, {X_corr.columns.values}\")\n","print(f\"Execution time: \\n{end_time - start_time}\")"]},{"cell_type":"markdown","metadata":{"id":"9U_nT6nkYacM"},"source":["## Mutual Information"]},{"cell_type":"markdown","metadata":{"id":"CR7HeoX2hFkW"},"source":["Mutual information of two attributes (features or variables) A and B is the amount of information shared between them. Or how much knowing A reduces the uncertainty (Entropy) about knowing B.\n","\n","Formula: $$ MI(A,B) = H(A) - H(A|B) $$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vzqrGRYAYIME"},"outputs":[],"source":["# Feature selection using mutual information\n","def mutual_info_selector(X, y, top_k=4):\n","    selector = SelectKBest(\n","        #FIX ME#,\n","        k=#FIX ME#)\n","    selector.fit(X, y)\n","    return X[X.columns[selector.get_support(indices=True)]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"V-rZiJIwZ-lf"},"outputs":[],"source":["# Feature selection using mutual information\n","print(f\"Initial features: \\n{X.columns.values}\")\n","#Set timer start\n","start_time = #FIX ME#\n","X_mi = mutual_info_selector(\n","    #FIX ME#,\n","    #FIX ME#,\n","    top_k=20)\n","#Set timer end\n","end_time = #FIX ME#\n","print(f\"Selected features: \\n{X_mi.columns.values}\")\n","print(f\"Execution time: \\n{end_time - start_time}\")\n"]},{"cell_type":"markdown","metadata":{"id":"XDRShVgwhFkX"},"source":["# Supervised feature selection methods"]},{"cell_type":"markdown","metadata":{"id":"8mI5uophYacO"},"source":["## Information Gain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"m6E7wGaTa6hf"},"outputs":[],"source":["# Feature selection using filter methods: information gain\n","def information_gain_selector(X, y, top_k=4):\n","    selector = SelectKBest(#FIX ME#, k=top_k)\n","    selector.fit(X, y)\n","    return X[X.columns[selector.get_support(indices=True)]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"giH8EKONbd3X"},"outputs":[],"source":["# Feature selection using information gain\n","print(f\"Initial features: \\n{X.columns.values}\")\n","#Set timer start\n","start_time = #FIX ME#\n","X_ig = information_gain_selector(X_train[categorical_features], y_train, top_k=20)\n","#Set timer start\n","end_time = #FIX ME#\n","print(f\"Selected features: \\n{X_ig.columns.values}\")\n","print(f\"Execution time: \\n{end_time - start_time}\")"]},{"cell_type":"markdown","metadata":{"id":"75YjmukJYacP"},"source":["## Chi-squared Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gK-3B-OobsF2"},"outputs":[],"source":["def chi_squared_selector(X, y, top_k=4):\n","    selector = SelectKBest(#FIX ME#, k=top_k)\n","    selector.fit(X, y)\n","    return X[X.columns[selector.get_support(indices=True)]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"1yvF2hOce5Nv"},"outputs":[],"source":["# Feature selection using chi-squared test\n","print(X.columns)\n","X_chi2 = chi_squared_selector(#FIX ME#, top_k=20)\n","print(X_chi2.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8Uf2ypaA4FNd"},"outputs":[],"source":["# Create a random forest classifier (you can replace this with any other classifier of your choice)\n","model = RandomForestClassifier(n_estimators=50, random_state=42, criterion=\"entropy\")"]},{"cell_type":"markdown","metadata":{"id":"OhMYddHF3obT"},"source":["## Feature Selection Using Wrapper Methods:"]},{"cell_type":"markdown","metadata":{"id":"bKx0a2xEYacR"},"source":["## Forward Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KTeqJjVUfHiL"},"outputs":[],"source":["# Feature selection using wrapper methods: forward selection\n","def forward_selection(X, y, model):\n","    sfs = SequentialFeatureSelector(\n","                                    #FIX ME#,\n","                                    k_features='best',\n","                                    #FIX ME#,\n","                                    #FIX ME#,\n","                                    cv=2)\n","    sfs.fit(X, y)\n","    return X[X.columns[list(sfs.k_feature_idx_)]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Wo9lpsh1f1TE"},"outputs":[],"source":["# Feature selection using forward selection\n","print(f\"Initial features: \\n{X.columns.values}\")\n","start_time =#FIX ME#\n","X_forward = forward_selection(X_train, y_train, model)\n","end_time = #FIX ME#\n","print(f\"Selected features: \\n{X_forward.columns.shape}, {X_forward.columns.values}\")\n","print(f\"Execution time: \\n{end_time - start_time}\")"]},{"cell_type":"markdown","metadata":{"id":"Y-nMMU6qYacS"},"source":["## Backward Elimination"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cZM5mvVMio4t"},"outputs":[],"source":["def backward_elimination(X, y, model):\n","    sfs = SequentialFeatureSelector(\n","        #FIX ME#,\n","        k_features='best',\n","        #FIX ME#,\n","        #FIX ME#,\n","        cv=2)\n","    sfs.fit(X, y)\n","    return X[X.columns[list(sfs.k_feature_idx_)]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"7KCNuhWLiyiV"},"outputs":[],"source":["# Feature selection using backward elimination\n","print(f\"Initial features: \\n{X.columns.values}\")\n","start_time = #FIX ME#\n","X_backward = #FIX ME#\n","end_time = #FIX ME#\n","print(f\"Selected features: \\n{X_backward.columns.shape}, {X_backward.columns.values}\")\n","print(f\"Execution time: \\n{end_time - start_time}\")"]},{"cell_type":"markdown","metadata":{"id":"60fZnrHaYacT"},"source":["## Genetic Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XDNI46SelPsv"},"outputs":[],"source":["def genetic_algorithm_selector(X, y, model):\n","    selector = GAFeatureSelectionCV(model,\n","                                         cv=2,\n","                                         scoring='accuracy',\n","                                         population_size=10,\n","                                         generations=5,\n","                                         tournament_size=5,\n","                                         elitism=True,\n","                                         crossover_probability=0.9,\n","                                         mutation_probability=0.1,\n","                                         criteria='max',\n","                                         algorithm='eaMuPlusLambda',\n","                                         n_jobs=3,\n","                                         verbose=True,\n","                                         keep_top_k=4)\n","\n","    selector.fit(X, y)\n","    return X[X.columns[selector.support_]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"xKVA5HSNlTla","scrolled":true},"outputs":[],"source":["print(f\"Initial features: \\n{X.columns.values}\")\n","start_time = time.time()\n","X_genetic = genetic_algorithm_selector(X_train, y_train, model)\n","end_time = time.time()\n","print(f\"Selected features: \\n{X_genetic.columns.shape}, {X_genetic.columns.values}\")\n","print(f\"Execution time: \\n{end_time - start_time}\")"]},{"cell_type":"markdown","metadata":{"id":"7bILaRlSYacU"},"source":["## --Evaluate Model performance after feature selection--"]},{"cell_type":"markdown","metadata":{"id":"4qA-d0e8YacG"},"source":["## evaluate model Function\n","\n","This function aims to evaluate a given model using Training and testing subsets. It is worth noting that we can also choose other metrics to evaluate the model's performance (comparison between predicted and true values)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8m3IXXup1k1z"},"outputs":[],"source":["def evaluate_model(model, X_train, X_test, y_train, y_test):\n","    model.fit(X_train, y_train) #Train the model using training sets\n","    y_pred = model.predict(X_test)#Predict using testing set\n","    accuracy = accuracy_score(y_test, y_pred)#Using Accuracy metric, Compare the predicted values with the testing target values\n","    return accuracy"]},{"cell_type":"markdown","metadata":{"id":"eUMf1Ykl4LU4"},"source":["## Compare model performance considering the selected features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"TEPO3SlqYacV"},"outputs":[],"source":["\n","# Concatenate the selected features of each method with the remaining non treated features (of the originial dataset)\n","X_variance = pd.concat(#FIX ME#)\n","X_correlation = pd.concat(#FIX ME#)\n","X_mInformation = pd.concat(#FIX ME#)\n","X_iGain = pd.concat(#FIX ME#)\n","X_chiSquare = pd.concat(#FIX ME#)\n","\n","selected_features = [#FIX ME#]\n","\n","selected_feature_names = ['Original', 'Variance Threshold', 'Correlation', 'Mutual Information', 'Information Gain', 'Chi-squared Test', 'Forward Selection', 'Backward Elimination', 'Genetic Algorithm']\n","\n","results = []\n","\n","for name, X_selected in zip(selected_feature_names, selected_features):\n","    X_test_selected = #FIX ME#  # Apply the same feature selection to the test set\n","    accuracy = evaluate_model(#FIX ME#)\n","    results.append({'Method': name, 'Accuracy': accuracy})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"t-9C4FVPYacW"},"outputs":[],"source":["# Display the results\n","results_df = pd.DataFrame(results)\n","print(results_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":564},"id":"ZWlfLrQhYacX"},"outputs":[],"source":["# Plot the results\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='Accuracy', y='Method', data=results_df.sort_values(by='Accuracy', ascending=False))\n","plt.title('Accuracy after Feature Selection')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GYvkhwE95ERE"},"source":["## **Q/A:** What do you notice ?"]},{"cell_type":"markdown","metadata":{"id":"GnStEEizTMHs"},"source":["## Built in features importance  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"DvbtWSxpOeS2"},"outputs":[],"source":["# Train the model on the whole feature set\n","model.fit(X_train, y_train)\n","\n","importances = #FIX ME# # Get importance features from RandomForest model (search for it)\n","feature_imp_df = pd.DataFrame(\n","                              {'Feature': #FIX ME#,\n","                               'Importance': #FIX ME#\n","                               }).sort_values('Importance', ascending=False)\n","\n","plt.figure(figsize=(10, 14))\n","plt.barh(X_train.columns.values, importances, color='skyblue')\n","plt.xlabel('Importance')\n","plt.title('Feature Importance - Entropy Importance')\n","plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"BLM6KUhypcJv"},"source":["## Hybrid Feature selection\n","\n","Think of a way to combine two feature selection methods, Perform it, and show the corresponding results."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PFrHOAf5pwG-"},"outputs":[],"source":["# FIX ME\n"]},{"cell_type":"markdown","metadata":{"id":"nPpJml0tW5Oo"},"source":["## Extra task\n","\n","Explore the application of PCA on the original sampled dataset;\n","\n","(1) Make sure 90% of variance is maintained.\n","\n","(2) Choose 2 principale components and plot the reduced data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aoA8h_ErChOh"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}